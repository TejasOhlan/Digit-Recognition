{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejasOhlan/Digit-Recognition/blob/main/Digit_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtfGBcMBWUn5"
      },
      "source": [
        "#Digit recognition system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k7zThXxZft3"
      },
      "source": [
        "##1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybxE2uN2SrhH"
      },
      "source": [
        "#loading required libraries\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfiknZl3dbpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eeccf4a-1c56-4181-93df-5da8073fce33"
      },
      "source": [
        "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) =tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVhyzwoFcEa"
      },
      "source": [
        "tensorflow : open-source library for number of various tasks in machine leaerning. </br> \n",
        "Keras : A high-level neural-network library written in python.</br><b>\n",
        "what does HIGH LEVEL mean here?</br></b>\n",
        "Sequential : it is a type of model in keras which is used when you have a single input and a single output in your model as well as in your layers. As the name suggests it works sequentially layer by layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YOrFGsHZVm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "02767d9d-e2f7-411a-aeb4-5e7153642566"
      },
      "source": [
        "plt.imshow(X_train[0])    # show first number in the dataset\n",
        "plt.show()\n",
        "print('Label: ', y_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC9Pn2VRZuZU"
      },
      "source": [
        "##2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ13edxfZ6y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478e016e-e53b-4d2c-edf6-333a35e71e15"
      },
      "source": [
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "#reshaping data\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "#(one-hot encoding in y_train and y_test)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n",
            "(60000, 784) (10000, 784) (60000, 10) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS9b97OaL0iP"
      },
      "source": [
        "Why do we reshape data- so that we can feed the data into the neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIf2Jg39dnWm"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS7-wPiOolLc"
      },
      "source": [
        "**Layers** are the basic building blocks of neural networks in Keras. A layer consists of a tensor-in tensor-out computation function.\n",
        "\n",
        "**Optimizers** tie together the loss function and model parameters by updating the model in response to the output of the loss function. </br>\n",
        "**In simpler terms**, optimizers shape and mold your model into its most accurate possible form by futzing with the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5MEKK9Sdw2k"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(50, input_shape = (784, )))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(50))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(50))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(50))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnOPMLtqVUWu"
      },
      "source": [
        "**Sigmoid**- The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.\n",
        "\n",
        "**Softmax** - The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. By definition, the softmax activation will output one value for each node in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_0GPm2eEdC"
      },
      "source": [
        "sgd = optimizers.SGD(learning_rate= 0.01)  #lr is learning rate\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJrZAYQUdrzD"
      },
      "source": [
        "### Basic NN model\n",
        "\n",
        "Naive MLP model without any alterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBumE8GHeH0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e2d561-c8b9-4ce2-e470-1422dd25268a"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 20, epochs = 100, verbose = 1)\n",
        "#verbose = 1 ... means it will show progress bar for each epoch\n",
        "#verbose = 0 ... means it will hide the progress of the model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3000/3000 [==============================] - 6s 2ms/step - loss: 2.2954 - accuracy: 0.1294\n",
            "Epoch 2/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 2.2535 - accuracy: 0.2225\n",
            "Epoch 3/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 1.9475 - accuracy: 0.3714\n",
            "Epoch 4/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 1.4263 - accuracy: 0.5181\n",
            "Epoch 5/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 1.1552 - accuracy: 0.6349\n",
            "Epoch 6/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.9984 - accuracy: 0.7044\n",
            "Epoch 7/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.9312 - accuracy: 0.7163\n",
            "Epoch 8/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.8993 - accuracy: 0.7193\n",
            "Epoch 9/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.8800 - accuracy: 0.7194\n",
            "Epoch 10/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.8415 - accuracy: 0.7404\n",
            "Epoch 11/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.7885 - accuracy: 0.7655\n",
            "Epoch 12/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.7825 - accuracy: 0.7650\n",
            "Epoch 13/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.7724 - accuracy: 0.7681\n",
            "Epoch 14/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.7600 - accuracy: 0.7639\n",
            "Epoch 15/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.7437 - accuracy: 0.7723\n",
            "Epoch 16/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6530 - accuracy: 0.8048\n",
            "Epoch 17/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6353 - accuracy: 0.8102\n",
            "Epoch 18/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6300 - accuracy: 0.8112\n",
            "Epoch 19/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6045 - accuracy: 0.8212\n",
            "Epoch 20/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6118 - accuracy: 0.8165\n",
            "Epoch 21/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.6061 - accuracy: 0.8141\n",
            "Epoch 22/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5999 - accuracy: 0.8192\n",
            "Epoch 23/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5840 - accuracy: 0.8275\n",
            "Epoch 24/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5565 - accuracy: 0.8388\n",
            "Epoch 25/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5567 - accuracy: 0.8332\n",
            "Epoch 26/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5462 - accuracy: 0.8348\n",
            "Epoch 27/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.5232 - accuracy: 0.8454\n",
            "Epoch 28/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4932 - accuracy: 0.8539\n",
            "Epoch 29/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4978 - accuracy: 0.8540\n",
            "Epoch 30/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4789 - accuracy: 0.8585\n",
            "Epoch 31/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4668 - accuracy: 0.8651\n",
            "Epoch 32/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4587 - accuracy: 0.8682\n",
            "Epoch 33/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4404 - accuracy: 0.8752\n",
            "Epoch 34/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4507 - accuracy: 0.8697\n",
            "Epoch 35/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4182 - accuracy: 0.8794\n",
            "Epoch 36/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4217 - accuracy: 0.8777\n",
            "Epoch 37/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4128 - accuracy: 0.8802\n",
            "Epoch 38/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4009 - accuracy: 0.8833\n",
            "Epoch 39/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4016 - accuracy: 0.8835\n",
            "Epoch 40/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4549 - accuracy: 0.8561\n",
            "Epoch 41/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.4316 - accuracy: 0.8749\n",
            "Epoch 42/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3976 - accuracy: 0.8850\n",
            "Epoch 43/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3929 - accuracy: 0.8838\n",
            "Epoch 44/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3916 - accuracy: 0.8862\n",
            "Epoch 45/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3845 - accuracy: 0.8882\n",
            "Epoch 46/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3628 - accuracy: 0.8954\n",
            "Epoch 47/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3637 - accuracy: 0.8941\n",
            "Epoch 48/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3631 - accuracy: 0.8918\n",
            "Epoch 49/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3563 - accuracy: 0.8952\n",
            "Epoch 50/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3637 - accuracy: 0.8930\n",
            "Epoch 51/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3531 - accuracy: 0.8977\n",
            "Epoch 52/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3534 - accuracy: 0.8975\n",
            "Epoch 53/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3494 - accuracy: 0.8990\n",
            "Epoch 54/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3510 - accuracy: 0.8970\n",
            "Epoch 55/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3512 - accuracy: 0.8965\n",
            "Epoch 56/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3502 - accuracy: 0.8976\n",
            "Epoch 57/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3450 - accuracy: 0.9007\n",
            "Epoch 58/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3505 - accuracy: 0.8971\n",
            "Epoch 59/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3444 - accuracy: 0.8988\n",
            "Epoch 60/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3378 - accuracy: 0.8997\n",
            "Epoch 61/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3292 - accuracy: 0.9015\n",
            "Epoch 62/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3200 - accuracy: 0.9062\n",
            "Epoch 63/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3198 - accuracy: 0.9064\n",
            "Epoch 64/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3208 - accuracy: 0.9069\n",
            "Epoch 65/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3491 - accuracy: 0.8953\n",
            "Epoch 66/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3439 - accuracy: 0.8989\n",
            "Epoch 67/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3277 - accuracy: 0.9038\n",
            "Epoch 68/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3154 - accuracy: 0.9069\n",
            "Epoch 69/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3222 - accuracy: 0.9044\n",
            "Epoch 70/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3317 - accuracy: 0.9024\n",
            "Epoch 71/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3367 - accuracy: 0.9000\n",
            "Epoch 72/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3297 - accuracy: 0.9010\n",
            "Epoch 73/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3187 - accuracy: 0.9058\n",
            "Epoch 74/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3143 - accuracy: 0.9071\n",
            "Epoch 75/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3003 - accuracy: 0.9120\n",
            "Epoch 76/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3172 - accuracy: 0.9064\n",
            "Epoch 77/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3105 - accuracy: 0.9082\n",
            "Epoch 78/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3164 - accuracy: 0.9048\n",
            "Epoch 79/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3034 - accuracy: 0.9098\n",
            "Epoch 80/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2936 - accuracy: 0.9135\n",
            "Epoch 81/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3001 - accuracy: 0.9115\n",
            "Epoch 82/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2948 - accuracy: 0.9115\n",
            "Epoch 83/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2814 - accuracy: 0.9158\n",
            "Epoch 84/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2999 - accuracy: 0.9086\n",
            "Epoch 85/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.3172 - accuracy: 0.9043\n",
            "Epoch 86/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2858 - accuracy: 0.9162\n",
            "Epoch 87/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2904 - accuracy: 0.9137\n",
            "Epoch 88/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2795 - accuracy: 0.9177\n",
            "Epoch 89/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2811 - accuracy: 0.9159\n",
            "Epoch 90/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2951 - accuracy: 0.9114\n",
            "Epoch 91/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2913 - accuracy: 0.9119\n",
            "Epoch 92/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2859 - accuracy: 0.9144\n",
            "Epoch 93/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2843 - accuracy: 0.9132\n",
            "Epoch 94/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2818 - accuracy: 0.9154\n",
            "Epoch 95/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2788 - accuracy: 0.9160\n",
            "Epoch 96/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2667 - accuracy: 0.9205\n",
            "Epoch 97/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2730 - accuracy: 0.9177\n",
            "Epoch 98/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2740 - accuracy: 0.9187\n",
            "Epoch 99/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2852 - accuracy: 0.9154\n",
            "Epoch 100/100\n",
            "3000/3000 [==============================] - 5s 2ms/step - loss: 0.2764 - accuracy: 0.9188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpNkxQ3yfOJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6606b53-ff56-4305-be86-ed6de3f47e76"
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.2767 - accuracy: 0.9190\n",
            "Test accuracy:  0.9190000295639038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4TnapXAnbjt"
      },
      "source": [
        "What is Batchnormalization?- epoch training time is redused by standardising the inputs</br> Relu/Sigmoid/Softmax :- what are these? </br> What is dropput ?- deleting or ignoring some neurons to prevent overfitting E.g a dog behind a pole</br>\n",
        "What is learning rate?</br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsCzHrVfh8x_"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDtBb2J8iIph"
      },
      "source": [
        "def mlp_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(50, input_shape = (784, ), kernel_initializer='RandomUniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(50, kernel_initializer='RandomUniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))    \n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(50, kernel_initializer='RandomUniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(50, kernel_initializer='RandomUniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10, kernel_initializer='RandomUniform'))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    adam = optimizers.Adam(lr = 0.01)\n",
        "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd932NpEnbVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dab4d2-7388-45b6-8556-e381fe0fd232"
      },
      "source": [
        "model = mlp_model()\n",
        "history = model.fit(X_train, y_train,batch_size=200, epochs = 10, verbose = 1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "300/300 [==============================] - 3s 6ms/step - loss: 0.6202 - accuracy: 0.8181\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.3750 - accuracy: 0.9007\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.3329 - accuracy: 0.9116\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.3022 - accuracy: 0.9206\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2886 - accuracy: 0.9243\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2826 - accuracy: 0.9265\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2725 - accuracy: 0.9277\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2624 - accuracy: 0.9315\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2503 - accuracy: 0.9336\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.2562 - accuracy: 0.9331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFjVrEWfnbSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db9f0f5-524b-4b72-c8ad-77401e3f3d90"
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1436 - accuracy: 0.9606\n",
            "Test accuracy:  0.9606000185012817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFZnxKiuYEV4"
      },
      "source": [
        "##save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9V5lWu2YHRG"
      },
      "source": [
        "#saving the model using keras in H5 format\n",
        "model.save('myDigitDetectionModel.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bweLVNtsYlsq"
      },
      "source": [
        "import os\n",
        "os.listdir()\n",
        "os.getcwd() #to know whist folder my program is in"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og64__aNsVQm"
      },
      "source": [
        "checking variations is accuracy by changing a parameter on by one and selecting the best result's parameter for next parameter's varaition.\n",
        "e.g fixing Lr 0.01 as this gets highest accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3iTXanF00iN"
      },
      "source": [
        "types of Initializer \n",
        "\n",
        "\n",
        "1.   Random Uniform : Samples are uniformly distributed over the half-open interval [low, high)... (includes low, but excludes high).e.g [-2,3)..it means equal\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZCyu2DRncFz"
      },
      "source": [
        "| Drop Out      | Lr     | Batch Size    | Initilizer | Epochs|  Accuracy |\n",
        "| :------------- | :----------: | ----------- | -----------:| -----------:| -----------:| \n",
        "|0.9 |0.001 |200 |he_normal |10 |0.17| \n",
        "|0.5 |0.001 |200 |he_normal |10 |0.85|\n",
        "|0.3 |0.001 |200 |he_normal |10 |0.85|\n",
        "|0.3 |0.00001|200|he_normal |10 |0.95|\n",
        "|0.3 |0.0001|200 |he_normal |10 |0.91|\n",
        "|0.3 |0.001 |200 |he_normal |10 |0.95|\n",
        "|0.3 |0.01  |200 |he_normal |10 |0.96|\n",
        "|0.3 |0.1   |200 |he_normal |10 |0.95|\n",
        "|0.3 |1     |200 |he_normal |10 |0.11|\n",
        "|0.3 |0.01  |2000|he_normal |10 |0.95|\n",
        "|0.3 |0.01  |20  |he_normal |10 |0.96|\n",
        "|0.3 |0.01  |2   |he_normal |10 |0.15|\n",
        "|0.3 |0.01  |200 |RandomUniform |10 |0.96|\n",
        "| | | | | | |\n",
        "| | | | | | |\n",
        "| | | | | | |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30B5uOqBcNYy"
      },
      "source": [
        "Fill this table with multiple parameters and check which combinations of parameters will give best accuracy. Also you will be able to know more types of paramters that can be used like for he_nomral what others are present."
      ]
    }
  ]
}